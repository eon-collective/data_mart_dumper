{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pip install pandas"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/james.kimani/Library/Python/3.10/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/james.kimani/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import argparse\n",
    "import traceback\n",
    "import hashlib\n",
    "\n",
    "def process_file(input_file_path):\n",
    "    try:\n",
    "        # base_input_dir = \"graph_data/nodes\"\n",
    "        # input_file = base_input_dir + \"/Databases.csv\"\n",
    "\n",
    "        df = pd.read_csv(input_file_path, sep=\",\", header=0, escapechar=\"\\\\\", quotechar=\"`\")\n",
    "        print(input_file_path +\" Read - The shape of dataframe: \"+ str(df.shape).replace(\",\", \" rows, with number of columns:\"))\n",
    "        print(df.loc[:, ['Entity_ID:ID','HashKey']])\n",
    "        deduped_df = df.drop_duplicates(subset=['HashKey'], keep='last').loc[df[\"Entity_Name\"] != \"Entity_Name\" ]\n",
    "        print(input_file_path + \" Deduped - The shape of dataframe: \"+ str(deduped_df.shape).replace(\",\", \" rows, with number of columns:\"))\n",
    "        print (deduped_df)\n",
    "        \n",
    "        os.remove(input_file_path)\n",
    "        print (input_file_path + \" Deleted - The file has been removed\")\n",
    "\n",
    "        from pathlib import Path  \n",
    "        filepath = Path(input_file_path)  \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "        deduped_df.to_csv(filepath, index=False, quotechar='`')\n",
    "        print (input_file_path + \" Recreated - The file has been recreated using deduped data\")\n",
    "        options = ['Entity_Name'] \n",
    "    \n",
    "        # selecting rows based on condition \n",
    "        rslt_df = deduped_df[deduped_df['Entity_Name'].isin(options)] \n",
    "            \n",
    "        print('\\nResult dataframe :\\n',\n",
    "            rslt_df)\n",
    "    except BaseException as e:\n",
    "        print (input_file_path + \"Failed - something went wrong in process_file for file \"+ str(e))\n",
    "\n",
    "        # traceback.print_exc\n",
    "        \n",
    "def profile_adept_from_folder(base_input_dir):\n",
    "    \"\"\"\n",
    "    profile_adept_results from a folder\n",
    "    \"\"\"\n",
    "    #os.system('pwd')\n",
    "    #os.system('ls -l')\n",
    "    now = datetime.datetime.now()\n",
    "    print (now)\n",
    "    try:\n",
    "        ddl_files = Path(base_input_dir).glob('*.csv')\n",
    "        dfs = list()\n",
    "        for f in ddl_files:\n",
    "            print(str(f) + \" found...\")\n",
    "            if 'Macros.csv' not in str(f):\n",
    "                print(\"Macro file found, skipping file from processing\"+str(f))\n",
    "            # dedupe each csv file found\n",
    "            process_file(str(f))\n",
    "    except Exception as e:\n",
    "        print (\"something went wrong in profile_adept_results during profile for a folder \"+ str(e))\n",
    "        raise\n",
    "    print (now)\n",
    "\n",
    "\n",
    "profile_adept_from_folder(\"/Users/Shared/ADEPT/out/DBT_TO_ADEPT/graph_data/nodes\")\n",
    "# process_file('/Users/Shared/ADEPT/out/DBT_TO_ADEPT/graph_data/nodes/Macros.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-08-21 19:06:36.156460\n",
      "2023-08-21 19:06:36.156460\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function: driver for application upload_harvester_output.py\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "                    prog='ADEPT Graph Data Deduper',\n",
    "                    description='''This program is used to test uploading to the ADEPT Console. This script will upload the output files generated by the harvester job.\n",
    "Refer: https://bitbucket.org/eoncollective-code-repository/adept-ui-console/src/master/docs/integration-notes/index.md\n",
    "for how to create nodes and relationships graph data with csv files.''',\n",
    "                    epilog='ADEPT utilities')\n",
    "    parser.add_argument('-i','--input',\n",
    "                    default='/Users/Shared/ADEPT/out/DBT_TO_ADEPT',\n",
    "                    required=True,\n",
    "                    help='Provide input location with .csv using a comma as delimiter (default)')\n",
    "    parser.add_argument('-t','--type',\n",
    "                        default='folder',\n",
    "                        choices=['file', 'folder'],\n",
    "                        required=True,\n",
    "                        help='Choice to process file of folder for dedupe')\n",
    "    args = parser.parse_args()\n",
    "    profile_adept_from_folder(args.input)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process One File with Graph Data for documentation in ADEPT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "input_file = \"/Users/Shared/ADEPT/out/100/graph_data/nodes/Users.csv\"\n",
    "print(input_file.split(\"/\")[-1].split(\".\")[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "output_dir=\"/Users/james.kimani/Development/repositories/data_mart_dumper/src/profiler\"\n",
    "job_name=\"output\"\n",
    "\n",
    "input_file = \"/Users/Shared/ADEPT/out/100/graph_data/nodes/Users.csv\"\n",
    "\n",
    "file_name =  Path(input_file).name\n",
    "file_extension = file_name.split(\".\")[1]\n",
    "\n",
    "df = pd.read_csv(input_file, sep=\",\", header=0, escapechar=\"\\\\\", quotechar=\"`\")\n",
    "\n",
    "# check if graph data directory for nodes exists, if not create it\n",
    "gd_nodes_directory=output_dir+'/'+job_name+'/graph_data/nodes'\n",
    "gd_nodes_directory_isExist = os.path.exists(gd_nodes_directory)\n",
    "if not gd_nodes_directory_isExist:\n",
    "    # Create a new directory because it does not exist for /graph_data/nodes\n",
    "    os.makedirs(gd_nodes_directory+\"/\")\n",
    "    print(\"The new directory is created! \"+ gd_nodes_directory)\n",
    "\n",
    "# check if graph data directory for nodes exists, if not create it\n",
    "gd_relationships_directory=output_dir+'/'+job_name+'/graph_data/relationships'\n",
    "gd_relationships_directory_isExist = os.path.exists(gd_relationships_directory)\n",
    "if not gd_relationships_directory_isExist:\n",
    "    # Create a new directory because it does not exist for /graph_data/relationships\n",
    "    os.makedirs(gd_relationships_directory+\"/\")\n",
    "    print(\"The new directory is created! \"+ gd_relationships_directory)\n",
    "\n",
    "is_NodeFile_Existing = os.path.exists(\"output/graph_data/nodes/file.csv\") \n",
    "if is_NodeFile_Existing == False:\n",
    "        BaseNodeFile = open(\"output/graph_data/nodes/file.csv\", \"a\")\n",
    "        # Schema => Entity_ID:ID,:Label,HashKey,name, Technical Data Type,\n",
    "        file_column_header_line = [\"Entity_ID:ID,:Label,HashKey,name,Path,Extension,Size,Last_Modified_Time,Creation_Time\\n\"]\n",
    "        BaseNodeFile.writelines(file_column_header_line)\n",
    "        file_column_line = [\"%s,%s,%s,%s,%s,%s,%s bytes,%s,%s\\n\"%(hashlib.md5((input_file).encode()).hexdigest(),\"File\",hashlib.md5((input_file).encode()).hexdigest(),file_name,input_file,file_extension,os.path.getsize(input_file),os.path.getctime(input_file),os.path.getmtime(input_file))]\n",
    "        BaseNodeFile.writelines(file_column_line)\n",
    "        BaseNodeFile.close()\n",
    "else:\n",
    "    BaseNodeFile = open(\"output/graph_data/nodes/file.csv\", \"a\")\n",
    "    # Schema => Entity_ID:ID,:Label,HashKey,name,Path,Extension,Size,Creation_Time,Last_Modified_Time\n",
    "    file_column_line = [\"%s,%s,%s,%s,%s,%s,%s bytes,%s,%s\\n\"%(hashlib.md5((input_file).encode()).hexdigest(),\"File\",hashlib.md5((input_file).encode()).hexdigest(),file_name,input_file,file_extension,os.path.getsize(input_file),os.path.getctime(input_file),os.path.getmtime(input_file))]\n",
    "    BaseNodeFile.writelines(file_column_line)\n",
    "    BaseNodeFile.close()\n",
    "          \n",
    "\n",
    "is_NodeFile_Existing = os.path.exists(\"output/graph_data/nodes/file_columns.csv\") \n",
    "if is_NodeFile_Existing == False:\n",
    "    BaseNodeFile = open(\"output/graph_data/nodes/file_columns.csv\", \"a\")\n",
    "    # Schema => Entity_ID:ID,:Label,HashKey,name, Technical Data Type,\n",
    "    file_column_line = [\"Entity_ID:ID,:Label,HashKey,name, Technical Data Type\\n\"]\n",
    "    BaseNodeFile.writelines(file_column_line)\n",
    "    BaseNodeFile.close()       \n",
    "\n",
    "is_RelationshipFile_Existing = os.path.exists(\"output/graph_data/relationships/relationships.csv\")\n",
    "if is_RelationshipFile_Existing == False:\n",
    "    BaseRelationshipFile = open(\"output/graph_data/relationships/relationships.csv\", \"a\")\n",
    "    # Schema => Entity_ID:ID,:Label,HashKey,name, Technical Data Type,\n",
    "    file_column_relationship_line = [\":START_ID,:END_ID,:TYPE,timestamp,Version\\n\"]\n",
    "    BaseRelationshipFile.writelines(file_column_relationship_line)\n",
    "    BaseRelationshipFile.close()     \n",
    "\n",
    "for colname, coltype in df.infer_objects().dtypes.items():       \n",
    "    print(colname, coltype)\n",
    "    NodeFileColumn = open(\"output/graph_data/nodes/file_columns.csv\", \"a\")\n",
    "    # Schema => Entity_ID:ID,:Label,HashKey,name, Technical Data Type,\n",
    "    Entity_ID = hashlib.md5((input_file+\"~\"+str(colname)).encode()).hexdigest()\n",
    "    Label = \"Column\"\n",
    "    HashKey = Entity_ID\n",
    "    name = str(colname)\n",
    "    Technical_Data_Type = str(coltype)\n",
    "    Source = filename\n",
    "    file_column_line = [Entity_ID + \",\" + Label + \",\" + HashKey + \",\" + name + \",\" + Technical_Data_Type + \",\" + Source + \"\\n\"]\n",
    "    NodeFileColumn.writelines(file_column_line)\n",
    "    NodeFileColumn.close()\n",
    "          \n",
    "    RelationshipFile = open(\"output/graph_data/relationships/relationships.csv\", \"a\")\n",
    "    # Schema => :START_ID,:END_ID,:TYPE,timestamp,Version\n",
    "    START_ID = hashlib.md5((input_file).encode()).hexdigest()\n",
    "    END_ID = hashlib.md5((input_file+\"~\"+str(colname)).encode()).hexdigest()\n",
    "    TYPE = \"contains_columns\"\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    Version = \"1\"\n",
    "    file_column_relationship_line = [START_ID + \",\" + END_ID + \",\" + TYPE + \",\" + timestamp + \",\" + Version + \"\\n\"]\n",
    "    RelationshipFile.writelines(file_column_relationship_line)\n",
    "    RelationshipFile.close()\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/graph_data/nodes/file.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m is_NodeFile_Existing \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/graph_data/nodes/file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_NodeFile_Existing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m         BaseNodeFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/graph_data/nodes/file.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Schema => Entity_ID:ID,:Label,HashKey,name, Technical Data Type,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         file_column_header_line \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity_ID:ID,:Label,HashKey,name,Path,Extension,Size,Last_Modified_Time,Creation_Time\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/graph_data/nodes/file.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# python for_loop_with_range_function.py\n",
    "\n",
    "str_list = [\"Macros.csv\", \"Models.csv\"]\n",
    "\n",
    "for x in range(len(str_list)):\n",
    "    print(str_list[x])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.11 64-bit"
  },
  "interpreter": {
   "hash": "8f9328efe3468e6c370cdfed98702d3986faf748314d5bcec59da615d65baa7a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}